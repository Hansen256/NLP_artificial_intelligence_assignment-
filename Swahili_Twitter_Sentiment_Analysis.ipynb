{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f998a09",
   "metadata": {},
   "source": [
    "# Swahili Twitter Sentiment Analysis: GRU vs AfriBERTa\n",
    "\n",
    "**Objective:** Compare a simple GRU model with a fine-tuned AfriBERTa transformer for Swahili tweet sentiment classification.\n",
    "\n",
    "**Contents:**\n",
    "1. Setup and Data Loading\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Preprocessing Pipeline\n",
    "4. GRU Model Implementation\n",
    "5. AfriBERTa Fine-Tuning\n",
    "6. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87a754",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading ðŸ“¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'datasets', 'transformers', 'tensorflow', 'scikit-learn',\n",
    "    'matplotlib', 'seaborn', 'pandas', 'numpy', 'tqdm'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"âœ“ All packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad710b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f824c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Swahili Twitter Sentiment dataset\n",
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    # Try loading the Swahili tweets dataset\n",
    "    dataset = load_dataset(\"swahili_tweets\", \"sentiment\")\n",
    "    train_data = pd.DataFrame(dataset['train'])\n",
    "    test_data = pd.DataFrame(dataset['test']) if 'test' in dataset else None\n",
    "except:\n",
    "    # Fallback: use tweet_eval dataset as demonstration\n",
    "    print(\"Using tweet_eval dataset as fallback...\")\n",
    "    dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "    train_full = pd.DataFrame(dataset['train'])\n",
    "    test_data = pd.DataFrame(dataset['test']) if 'test' in dataset else None\n",
    "    \n",
    "    # Rename columns for consistency\n",
    "    if 'text' not in train_full.columns:\n",
    "        train_full = train_full.rename(columns={'Tweet': 'text'})\n",
    "    if 'label' not in train_full.columns:\n",
    "        train_full = train_full.rename(columns={'Label': 'label'})\n",
    "    \n",
    "    # Split into train/val\n",
    "    train_data, val_data = train_test_split(train_full, test_size=0.2, random_state=42, stratify=train_full['label'])\n",
    "\n",
    "# Ensure consistent column names\n",
    "if 'tweet' in train_data.columns:\n",
    "    train_data = train_data.rename(columns={'tweet': 'text'})\n",
    "if 'sentiment' in train_data.columns:\n",
    "    train_data = train_data.rename(columns={'sentiment': 'label'})\n",
    "\n",
    "print(f\"\\nâœ“ Dataset loaded: {len(train_data)} training samples\")\n",
    "print(f\"âœ“ Features: {list(train_data.columns)}\")\n",
    "print(f\"\\nFirst 2 samples:\")\n",
    "print(train_data.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde52c0",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA) ðŸ“Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Dataset statistics and visualization\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Text length statistics\n",
    "train_data['text_length'] = train_data['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(f\"  Mean: {train_data['text_length'].mean():.1f} words\")\n",
    "print(f\"  Median: {train_data['text_length'].median():.1f} words\")\n",
    "print(f\"  Max: {train_data['text_length'].max()} words\")\n",
    "print(f\"  Min: {train_data['text_length'].min()} words\")\n",
    "\n",
    "# Label distribution\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "label_counts = train_data['label'].value_counts().sort_index()\n",
    "sentiment_names = ['Negative', 'Neutral', 'Positive']\n",
    "for label_idx, count in label_counts.items():\n",
    "    pct = 100 * count / len(train_data)\n",
    "    print(f\"  {sentiment_names[label_idx]:10} (Label {label_idx}): {int(count):5} ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal samples: {len(train_data)}\")\n",
    "\n",
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Bar chart\n",
    "colors = ['#FF6B6B', '#FFA500', '#4ECB71']\n",
    "axes[0].bar(sentiment_names, label_counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Sentiment Label Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(label_counts, labels=sentiment_names, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "axes[1].set_title('Sentiment Distribution (%)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text length distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(train_data['text_length'], bins=50, color='#4ECDC4', alpha=0.7, edgecolor='black')\n",
    "ax.axvline(train_data['text_length'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {train_data[\"text_length\"].mean():.1f}')\n",
    "ax.axvline(train_data['text_length'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {train_data[\"text_length\"].median():.1f}')\n",
    "ax.set_title('Distribution of Text Lengths', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Number of Words')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ EDA complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a1026",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Pipeline ðŸ§¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b5795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove URLs, mentions, and special characters\"\"\"\n",
    "    text = re.sub(r'http\\\\S+|www\\\\S+|https\\\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'@\\\\w+', '', text)  # Remove @mentions\n",
    "    text = re.sub(r'#', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^A-Za-z0-9\\\\s]', '', text)  # Remove special characters\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Swahili stopwords\n",
    "swahili_stopwords = {\n",
    "    'na', 'ni', 'kwa', 'za', 'wa', 'kwamba', 'ile', 'hii', 'hiyo', 'yule',\n",
    "    'mwenyewe', 'wao', 'zao', 'kwenye', 'karibu', 'pamoja', 'sana', 'tu',\n",
    "    'zaidi', 'kidogo', 'tena', 'kila', 'nyingine', 'ingine', 'pia', 'au',\n",
    "    'lakini', 'kama', 'ikiwa', 'akiwa', 'wakati', 'kabla', 'baada', 'tangu'\n",
    "}\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove Swahili stopwords\"\"\"\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t.lower() not in swahili_stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing text...\")\n",
    "train_data['text_cleaned'] = train_data['text'].apply(lambda x: clean_text(str(x)))\n",
    "train_data['text_cleaned'] = train_data['text_cleaned'].apply(remove_stopwords)\n",
    "\n",
    "print(\"âœ“ Text cleaned and stopwords removed\")\n",
    "print(f\"\\nExample cleaned texts:\")\n",
    "for i in range(2):\n",
    "    print(f\"  Original: {train_data['text'].iloc[i][:80]}...\")\n",
    "    print(f\"  Cleaned:  {train_data['text_cleaned'].iloc[i][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/validation/test for GRU model\n",
    "# Use 60% train, 20% val, 20% test\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    range(len(train_data)), test_size=0.4, random_state=42, \n",
    "    stratify=train_data['label']\n",
    ")\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=0.5, random_state=42,\n",
    "    stratify=train_data.iloc[temp_idx]['label']\n",
    ")\n",
    "\n",
    "X_train = train_data.iloc[train_idx]['text_cleaned'].values\n",
    "y_train = train_data.iloc[train_idx]['label'].values\n",
    "\n",
    "X_val = train_data.iloc[val_idx]['text_cleaned'].values\n",
    "y_val = train_data.iloc[val_idx]['label'].values\n",
    "\n",
    "X_test = train_data.iloc[test_idx]['text_cleaned'].values\n",
    "y_test = train_data.iloc[test_idx]['label'].values\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "# Tokenization and padding for GRU\n",
    "MAX_VOCAB = 5000\n",
    "MAX_LEN = 100\n",
    "\n",
    "tokenizer_gru = Tokenizer(num_words=MAX_VOCAB, oov_token='<unk>')\n",
    "tokenizer_gru.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer_gru.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer_gru.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer_gru.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "print(f\"âœ“ Tokenization & padding complete\")\n",
    "print(f\"  Vocab size: {len(tokenizer_gru.word_index) + 1}\")\n",
    "print(f\"  Sequence shape: {X_train_pad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab73d42",
   "metadata": {},
   "source": [
    "## 4. GRU Model Implementation ðŸ§ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75eb7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train GRU model\n",
    "print(\"Building GRU model...\")\n",
    "\n",
    "gru_model = Sequential([\n",
    "    Embedding(input_dim=MAX_VOCAB, output_dim=64, input_length=MAX_LEN),\n",
    "    GRU(units=64, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "gru_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(gru_model.summary())\n",
    "\n",
    "# Train GRU model\n",
    "print(\"\\nTraining GRU model...\")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "gru_history = gru_model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_data=(X_val_pad, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"âœ“ GRU training complete\")\n",
    "\n",
    "# Plot GRU training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(gru_history.history['accuracy'], label='Train Accuracy')\n",
    "axes[0].plot(gru_history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[0].set_title('GRU Model Accuracy', fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(gru_history.history['loss'], label='Train Loss')\n",
    "axes[1].plot(gru_history.history['val_loss'], label='Val Loss')\n",
    "axes[1].set_title('GRU Model Loss', fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41493efb",
   "metadata": {},
   "source": [
    "## 5. AfriBERTa Fine-Tuning âš¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for AfriBERTa (using transformer tokenizer)\n",
    "print(\"Preparing data for AfriBERTa...\")\n",
    "\n",
    "# Use a small pretrained model for fast training\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenize texts for BERT\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer_bert(\n",
    "        texts, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=128,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "# Tokenize train, val, and test sets\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_encodings = tokenize_function(X_train.tolist())\n",
    "val_encodings = tokenize_function(X_val.tolist())\n",
    "test_encodings = tokenize_function(X_test.tolist())\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings), \n",
    "    y_train\n",
    ")).batch(16).shuffle(100)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings), \n",
    "    y_val\n",
    ")).batch(16)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings), \n",
    "    y_test\n",
    ")).batch(16)\n",
    "\n",
    "print(\"âœ“ Data prepared for AfriBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5469b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and fine-tune AfriBERTa model\n",
    "print(\"Loading AfriBERTa model...\")\n",
    "\n",
    "bert_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "bert_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train AfriBERTa\n",
    "print(\"Training AfriBERTa model...\")\n",
    "bert_history = bert_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"âœ“ AfriBERTa training complete\")\n",
    "\n",
    "# Plot AfriBERTa training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(bert_history.history['accuracy'], label='Train Accuracy')\n",
    "axes[0].plot(bert_history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[0].set_title('AfriBERTa Model Accuracy', fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(bert_history.history['loss'], label='Train Loss')\n",
    "axes[1].plot(bert_history.history['val_loss'], label='Val Loss')\n",
    "axes[1].set_title('AfriBERTa Model Loss', fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f478e1",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation & Comparison ðŸ“Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e6f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "print(\"Generating predictions...\")\n",
    "\n",
    "# GRU predictions\n",
    "gru_pred_probs = gru_model.predict(X_test_pad, verbose=0)\n",
    "gru_pred = np.argmax(gru_pred_probs, axis=1)\n",
    "\n",
    "# AfriBERTa predictions\n",
    "bert_pred_probs = bert_model.predict(test_dataset, verbose=0)\n",
    "bert_pred = np.argmax(bert_pred_probs.logits, axis=1)\n",
    "\n",
    "print(\"âœ“ Predictions generated\")\n",
    "\n",
    "# Calculate metrics for both models\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    return accuracy, precision, f1\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gru_metrics = calculate_metrics(y_test, gru_pred, \"GRU Model\")\n",
    "bert_metrics = calculate_metrics(y_test, bert_pred, \"AfriBERTa Model\")\n",
    "\n",
    "# Determine best model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "best_model_name = \"AfriBERTa\" if bert_metrics[0] > gru_metrics[0] else \"GRU\"\n",
    "best_pred = bert_pred if bert_metrics[0] > gru_metrics[0] else gru_pred\n",
    "best_metrics = bert_metrics if bert_metrics[0] > gru_metrics[0] else gru_metrics\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Accuracy: {best_metrics[0]:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4323eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix for best model\n",
    "print(f\"\\nDetailed Classification Report ({best_model_name}):\")\n",
    "print(classification_report(y_test, best_pred, target_names=['Negative', 'Neutral', 'Positive']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, best_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "            yticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            ax=ax)\n",
    "ax.set_title(f'Confusion Matrix - {best_model_name} Model', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('True Label', fontsize=11)\n",
    "ax.set_xlabel('Predicted Label', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd53624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = ['GRU', 'AfriBERTa']\n",
    "accuracy_scores = [gru_metrics[0], bert_metrics[0]]\n",
    "precision_scores = [gru_metrics[1], bert_metrics[1]]\n",
    "f1_scores = [gru_metrics[2], bert_metrics[2]]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, accuracy_scores, width, label='Accuracy', color='#4ECDC4', alpha=0.8)\n",
    "bars2 = ax.bar(x, precision_scores, width, label='Precision', color='#FFA500', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, f1_scores, width, label='F1-Score', color='#FF6B6B', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Model', fontweight='bold')\n",
    "ax.set_ylabel('Score', fontweight='bold')\n",
    "ax.set_title('Model Comparison: Key Metrics', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Analysis complete!\")\n",
    "print(f\"\\nFinal Summary:\")\n",
    "print(f\"  Best Model: {best_model_name}\")\n",
    "print(f\"  Best Accuracy: {best_metrics[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ba7a8",
   "metadata": {},
   "source": [
    "# Swahili Twitter Sentiment Analysis: GRU vs AfriBERTa\n",
    "\n",
    "**Objective:** Compare a simple GRU model with a fine-tuned AfriBERTa transformer for Swahili tweet sentiment classification.\n",
    "\n",
    "**Contents:**\n",
    "1. Setup and Data Loading\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Preprocessing Pipeline\n",
    "4. GRU Model Implementation\n",
    "5. AfriBERTa Fine-Tuning\n",
    "6. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc0a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Swahili Twitter Sentiment dataset\n",
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    # Try loading the Swahili tweets dataset\n",
    "    dataset = load_dataset(\"swahili_tweets\", \"sentiment\")\n",
    "    train_data = pd.DataFrame(dataset['train'])\n",
    "    test_data = pd.DataFrame(dataset['test']) if 'test' in dataset else None\n",
    "except:\n",
    "    # Fallback: use tweet_eval dataset as demonstration\n",
    "    print(\"Using tweet_eval dataset as fallback...\")\n",
    "    dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "    train_full = pd.DataFrame(dataset['train'])\n",
    "    test_data = pd.DataFrame(dataset['test']) if 'test' in dataset else None\n",
    "    \n",
    "    # Rename columns for consistency\n",
    "    if 'text' not in train_full.columns:\n",
    "        train_full = train_full.rename(columns={'Tweet': 'text'})\n",
    "    if 'label' not in train_full.columns:\n",
    "        train_full = train_full.rename(columns={'Label': 'label'})\n",
    "    \n",
    "    # Split into train/val\n",
    "    train_data, val_data = train_test_split(train_full, test_size=0.2, random_state=42, stratify=train_full['label'])\n",
    "\n",
    "# Ensure consistent column names\n",
    "if 'tweet' in train_data.columns:\n",
    "    train_data = train_data.rename(columns={'tweet': 'text'})\n",
    "if 'sentiment' in train_data.columns:\n",
    "    train_data = train_data.rename(columns={'sentiment': 'label'})\n",
    "\n",
    "print(f\"\\nâœ“ Dataset loaded: {len(train_data)} training samples\")\n",
    "print(f\"âœ“ Features: {list(train_data.columns)}\")\n",
    "print(f\"\\nFirst 2 samples:\")\n",
    "print(train_data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f59d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/validation/test for GRU model\n",
    "# Use 60% train, 20% val, 20% test\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    range(len(train_data)), test_size=0.4, random_state=42, \n",
    "    stratify=train_data['label']\n",
    ")\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=0.5, random_state=42,\n",
    "    stratify=train_data.iloc[temp_idx]['label']\n",
    ")\n",
    "\n",
    "X_train = train_data.iloc[train_idx]['text_cleaned'].values\n",
    "y_train = train_data.iloc[train_idx]['label'].values\n",
    "\n",
    "X_val = train_data.iloc[val_idx]['text_cleaned'].values\n",
    "y_val = train_data.iloc[val_idx]['label'].values\n",
    "\n",
    "X_test = train_data.iloc[test_idx]['text_cleaned'].values\n",
    "y_test = train_data.iloc[test_idx]['label'].values\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "# Tokenization and padding for GRU\n",
    "MAX_VOCAB = 5000\n",
    "MAX_LEN = 100\n",
    "\n",
    "tokenizer_gru = Tokenizer(num_words=MAX_VOCAB, oov_token='<unk>')\n",
    "tokenizer_gru.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer_gru.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer_gru.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer_gru.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "print(f\"âœ“ Tokenization & padding complete\")\n",
    "print(f\"  Vocab size: {len(tokenizer_gru.word_index) + 1}\")\n",
    "print(f\"  Sequence shape: {X_train_pad.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = ['GRU', 'AfriBERTa']\n",
    "accuracy_scores = [gru_metrics[0], bert_metrics[0]]\n",
    "precision_scores = [gru_metrics[1], bert_metrics[1]]\n",
    "f1_scores = [gru_metrics[2], bert_metrics[2]]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, accuracy_scores, width, label='Accuracy', color='#4ECDC4', alpha=0.8)\n",
    "bars2 = ax.bar(x, precision_scores, width, label='Precision', color='#FFA500', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, f1_scores, width, label='F1-Score', color='#FF6B6B', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Model', fontweight='bold')\n",
    "ax.set_ylabel('Score', fontweight='bold')\n",
    "ax.set_title('Model Comparison: Key Metrics', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Analysis complete!\")\n",
    "print(f\"\\nFinal Summary:\")\n",
    "print(f\"  Best Model: {best_model_name}\")\n",
    "print(f\"  Best Accuracy: {best_metrics[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix for best model\n",
    "print(f\"\\nDetailed Classification Report ({best_model_name}):\")\n",
    "print(classification_report(y_test, best_pred, target_names=['Negative', 'Neutral', 'Positive']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, best_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "            yticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            ax=ax)\n",
    "ax.set_title(f'Confusion Matrix - {best_model_name} Model', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('True Label', fontsize=11)\n",
    "ax.set_xlabel('Predicted Label', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "print(\"Generating predictions...\")\n",
    "\n",
    "# GRU predictions\n",
    "gru_pred_probs = gru_model.predict(X_test_pad, verbose=0)\n",
    "gru_pred = np.argmax(gru_pred_probs, axis=1)\n",
    "\n",
    "# AfriBERTa predictions\n",
    "bert_pred_probs = bert_model.predict(test_dataset, verbose=0)\n",
    "bert_pred = np.argmax(bert_pred_probs.logits, axis=1)\n",
    "\n",
    "print(\"âœ“ Predictions generated\")\n",
    "\n",
    "# Calculate metrics for both models\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    return accuracy, precision, f1\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gru_metrics = calculate_metrics(y_test, gru_pred, \"GRU Model\")\n",
    "bert_metrics = calculate_metrics(y_test, bert_pred, \"AfriBERTa Model\")\n",
    "\n",
    "# Determine best model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "best_model_name = \"AfriBERTa\" if bert_metrics[0] > gru_metrics[0] else \"GRU\"\n",
    "best_pred = bert_pred if bert_metrics[0] > gru_metrics[0] else gru_pred\n",
    "best_metrics = bert_metrics if bert_metrics[0] > gru_metrics[0] else gru_metrics\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Accuracy: {best_metrics[0]:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3a9a4",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation & Comparison ðŸ“Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9954af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and fine-tune AfriBERTa model\n",
    "print(\"Loading AfriBERTa model...\")\n",
    "\n",
    "bert_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "bert_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train AfriBERTa\n",
    "print(\"Training AfriBERTa model...\")\n",
    "bert_history = bert_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"âœ“ AfriBERTa training complete\")\n",
    "\n",
    "# Plot AfriBERTa training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(bert_history.history['accuracy'], label='Train Accuracy')\n",
    "axes[0].plot(bert_history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[0].set_title('AfriBERTa Model Accuracy', fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(bert_history.history['loss'], label='Train Loss')\n",
    "axes[1].plot(bert_history.history['val_loss'], label='Val Loss')\n",
    "axes[1].set_title('AfriBERTa Model Loss', fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d5e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for AfriBERTa (using transformer tokenizer)\n",
    "print(\"Preparing data for AfriBERTa...\")\n",
    "\n",
    "# Use a small pretrained model for fast training\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenize texts for BERT\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer_bert(\n",
    "        texts, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=128,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "# Tokenize train, val, and test sets\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_encodings = tokenize_function(X_train.tolist())\n",
    "val_encodings = tokenize_function(X_val.tolist())\n",
    "test_encodings = tokenize_function(X_test.tolist())\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings), \n",
    "    y_train\n",
    ")).batch(16).shuffle(100)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings), \n",
    "    y_val\n",
    ")).batch(16)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings), \n",
    "    y_test\n",
    ")).batch(16)\n",
    "\n",
    "print(\"âœ“ Data prepared for AfriBERTa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2405dca3",
   "metadata": {},
   "source": [
    "## 5. AfriBERTa Fine-Tuning âš¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f54e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train GRU model\n",
    "print(\"Building GRU model...\")\n",
    "\n",
    "gru_model = Sequential([\n",
    "    Embedding(input_dim=MAX_VOCAB, output_dim=64, input_length=MAX_LEN),\n",
    "    GRU(units=64, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "gru_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(gru_model.summary())\n",
    "\n",
    "# Train GRU model\n",
    "print(\"\\nTraining GRU model...\")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "gru_history = gru_model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_data=(X_val_pad, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"âœ“ GRU training complete\")\n",
    "\n",
    "# Plot GRU training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(gru_history.history['accuracy'], label='Train Accuracy')\n",
    "axes[0].plot(gru_history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[0].set_title('GRU Model Accuracy', fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(gru_history.history['loss'], label='Train Loss')\n",
    "axes[1].plot(gru_history.history['val_loss'], label='Val Loss')\n",
    "axes[1].set_title('GRU Model Loss', fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c496c90",
   "metadata": {},
   "source": [
    "## 4. GRU Model Implementation ðŸ§ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00626153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove URLs, mentions, and special characters\"\"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove @mentions\n",
    "    text = re.sub(r'#', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Swahili stopwords\n",
    "swahili_stopwords = {\n",
    "    'na', 'ni', 'kwa', 'za', 'wa', 'kwamba', 'ile', 'hii', 'hiyo', 'yule',\n",
    "    'mwenyewe', 'wao', 'zao', 'kwenye', 'karibu', 'pamoja', 'sana', 'tu',\n",
    "    'zaidi', 'kidogo', 'tena', 'kila', 'nyingine', 'ingine', 'pia', 'au',\n",
    "    'lakini', 'kama', 'ikiwa', 'akiwa', 'wakati', 'kabla', 'baada', 'tangu'\n",
    "}\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove Swahili stopwords\"\"\"\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t.lower() not in swahili_stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing text...\")\n",
    "train_data['text_cleaned'] = train_data['text'].apply(lambda x: clean_text(str(x)))\n",
    "train_data['text_cleaned'] = train_data['text_cleaned'].apply(remove_stopwords)\n",
    "\n",
    "print(\"âœ“ Text cleaned and stopwords removed\")\n",
    "print(f\"\\nExample cleaned texts:\")\n",
    "for i in range(2):\n",
    "    print(f\"  Original: {train_data['text'].iloc[i][:80]}...\")\n",
    "    print(f\"  Cleaned:  {train_data['text_cleaned'].iloc[i][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d276e",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Pipeline ðŸ§¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Dataset statistics and visualization\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Text length statistics\n",
    "train_data['text_length'] = train_data['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(f\"  Mean: {train_data['text_length'].mean():.1f} words\")\n",
    "print(f\"  Median: {train_data['text_length'].median():.1f} words\")\n",
    "print(f\"  Max: {train_data['text_length'].max()} words\")\n",
    "print(f\"  Min: {train_data['text_length'].min()} words\")\n",
    "\n",
    "# Label distribution\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "label_counts = train_data['label'].value_counts().sort_index()\n",
    "sentiment_names = ['Negative', 'Neutral', 'Positive']\n",
    "for label_idx, count in label_counts.items():\n",
    "    pct = 100 * count / len(train_data)\n",
    "    print(f\"  {sentiment_names[label_idx]:10} (Label {label_idx}): {int(count):5} ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal samples: {len(train_data)}\")\n",
    "\n",
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Bar chart\n",
    "colors = ['#FF6B6B', '#FFA500', '#4ECB71']\n",
    "axes[0].bar(sentiment_names, label_counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Sentiment Label Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(label_counts, labels=sentiment_names, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "axes[1].set_title('Sentiment Distribution (%)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text length distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(train_data['text_length'], bins=50, color='#4ECDC4', alpha=0.7, edgecolor='black')\n",
    "ax.axvline(train_data['text_length'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {train_data[\"text_length\"].mean():.1f}')\n",
    "ax.axvline(train_data['text_length'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {train_data[\"text_length\"].median():.1f}')\n",
    "ax.set_title('Distribution of Text Lengths', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Number of Words')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ EDA complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76ca684",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA) ðŸ“Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b688a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, pipeline\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f061513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'datasets', 'transformers', 'tensorflow', 'scikit-learn',\n",
    "    'matplotlib', 'seaborn', 'pandas', 'numpy', 'tqdm'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"âœ“ All packages installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b2b4b6",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading ðŸ“¥"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
